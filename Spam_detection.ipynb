# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV

# Load the dataset
df = pd.read_csv('Task1.csv')

# Display basic information about the dataset
print(df.head())
print(df.info())

# Check for missing values
print(df.isnull().sum())

# Drop rows with missing values
df = df.dropna()

# Encode labels
le = LabelEncoder()
df['label'] = le.fit_transform(df['label'])

# Check dataset balance
print(df['label'].value_counts())
# Initialize the TfidfVectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the text data
X = vectorizer.fit_transform(df['text'])
y = df['label']
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Logistic Regression model
model = LogisticRegression(max_iter=1000)

# Train the model
model.fit(X_train, y_train)
# Predict on the test set
y_pred = model.predict(X_test)

# Print accuracy
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')

# Print confusion matrix
print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))

# Print classification report
print('Classification Report:')
print(classification_report(y_test, y_pred, target_names=le.classes_))
# Define the parameter grid for tuning
param_grid = {
    'C': [0.1, 1, 10, 100],
    'solver': ['liblinear', 'saga']
}

# Initialize GridSearchCV
grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, n_jobs=-1, verbose=1)

# Perform grid search
grid_search.fit(X_train, y_train)

# Print best parameters and best score
print(f'Best Parameters: {grid_search.best_params_}')
print(f'Best Score: {grid_search.best_score_}')

# Use the best model to predict on the test set
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)

# Print accuracy with the best model
print(f'Accuracy with Best Model: {accuracy_score(y_test, y_pred_best)}')

# Print confusion matrix with the best model
print('Confusion Matrix with Best Model:')
print(confusion_matrix(y_test, y_pred_best))

# Print classification report with the best model
print('Classification Report with Best Model:')
print(classification_report(y_test, y_pred_best, target_names=le.classes_))
import joblib

# Save the model to a file
joblib.dump(model, 'spam_detection_model.pkl')

# To load the model later
# loaded_model = joblib.load('spam_detection_model.pkl')
